{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "import numpy as np\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "def loadMovieReviews():\n",
    "    df = pd.read_csv('IMDB Reviews.csv')\n",
    "    return df['Reviews'], df['Sentiments']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze(clf, vectorizer):\n",
    "    names = np.asarray(vectorizer.get_feature_names())\n",
    "    w = np.argsort(clf.coef_.squeeze())\n",
    "\n",
    "    #the most positives features have big positive values\n",
    "    print ('Most Positive sentiment words', np.asarray(names)[w[-10:]])\n",
    "    # the most negatives features have big negative values\n",
    "    print ('Most Negative sentiment words', np.asarray(names)[w[:10]])\n",
    "    # Most unusefull words have values around zero\n",
    "    print ('Most unusefull words', names[np.argsort(np.abs(clf.coef_.squeeze()))[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(revTrain, revTest, sentTrain, sentTest, ngram_range):\n",
    "    tokenizer = nltk.RegexpTokenizer(r'[a-zA-Z]{2,}')               # extact only words of two characters and more\n",
    "    vectorizer = TfidfVectorizer(analyzer=\"word\",\n",
    "                                 # remove HTML tags and convert words to lower case\n",
    "                                 preprocessor=lambda w: BeautifulSoup(w, 'lxml').get_text().lower(),\n",
    "                                 tokenizer=tokenizer.tokenize,\n",
    "                                 stop_words=nltk.corpus.stopwords.words('english'),  # words to be removed\n",
    "                                 lowercase=False,  # not need it as we already convert them\n",
    "                                 ngram_range=ngram_range,  # unigram\n",
    "                                 min_df=2,              #eleminiate words that only apear in one review\n",
    "                                 max_df=int(80.0 * len(revTrain) / 100), #eleminiate words that apear most of the reviews\n",
    "                                 )\n",
    "\n",
    "    matTrain = vectorizer.fit_transform(revTrain)       #convert reviews to matrix if numeric values\n",
    "    matTest = vectorizer.transform(revTest)             #do the same for test\n",
    "    clf = LinearSVC()\n",
    "    clf.fit(matTrain, sentTrain)                        #train the data with linear SVM\n",
    "    print('Test accuracy {0}'.format(clf.score(matTest, sentTest)))\n",
    "    analyze(clf, vectorizer)\n",
    "    with open('svm.pkl', 'wb') as f:\n",
    "        pickle.dump(clf,f)\n",
    "    with open('vectorizer.pkl', 'wb') as f:\n",
    "        pickle.dump((vectorizer.vocabulary_, vectorizer.idf_),f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews, sentiments = loadMovieReviews()\n",
    "revTrain, revTest, sentTrain, sentTest = train_test_split(reviews, sentiments, test_size=0.15, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy 0.8981333333333333\n",
      "Most Positive sentiment words ['enjoyable' 'hilarious' 'best' 'wonderful' 'funniest' 'refreshing'\n",
      " 'amazing' 'perfect' 'great' 'excellent']\n",
      "Most Negative sentiment words ['worst' 'waste' 'awful' 'disappointment' 'boring' 'fails' 'terrible'\n",
      " 'poor' 'bad' 'disappointing']\n",
      "Most unusefull words ['caspar' 'amelio' 'kep' 'tactically' 'reefs' 'digusting' 'kester' 'keusch'\n",
      " 'reductivist' 'redlitch']\n"
     ]
    }
   ],
   "source": [
    "model(revTrain, revTest, sentTrain, sentTest, (1,1)) #uni-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy 0.9142666666666667\n",
      "Most Positive sentiment words ['favorite' 'today' 'best' 'hilarious' 'brilliant' 'wonderful' 'amazing'\n",
      " 'perfect' 'great' 'excellent']\n",
      "Most Negative sentiment words ['worst' 'awful' 'boring' 'bad' 'waste' 'terrible' 'poor' 'disappointment'\n",
      " 'nothing' 'dull']\n",
      "Most unusefull words ['huge responsibility' 'trappist monk' 'overall please'\n",
      " 'given unfortunately' 'trappist' 'appearance cute' 'films hey' 'demonico'\n",
      " 'crap said' 'unendingly']\n"
     ]
    }
   ],
   "source": [
    "model(revTrain, revTest, sentTrain, sentTest, (1, 2)) #bi-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# import sqlite3\n",
    "# import time\n",
    "# import ssl\n",
    "# import urllib\n",
    "# from urlparse import urljoin\n",
    "# from urlparse import urlparse\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "import string\n",
    "import zlib\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "import scipy.sparse as sp\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "from datetime import date\n",
    "\n",
    "imdb = pd.read_csv('Black Panther.csv', encoding='latin-1')\n",
    "\n",
    "\n",
    "class MyVectorizer(TfidfVectorizer):\n",
    "    '''\n",
    "    it is dummy class to workaround sklearn limitation\n",
    "    '''\n",
    "    def setIDF(self, idf):\n",
    "        TfidfVectorizer.idf_ =idf\n",
    "\n",
    "class SentimentAnalyzer:\n",
    "    def __init__(self):\n",
    "        with open('svm.pkl','rb') as f:\n",
    "            self.clf = pickle.load(f)\n",
    "\n",
    "        tokenizer = nltk.RegexpTokenizer(r'[a-zA-Z]{2,}')  # extact only words of two characters and more\n",
    "        self.vectorizer = MyVectorizer(analyzer=\"word\",\n",
    "                                 # remove HTML tags and convert words to lower case\n",
    "                                 preprocessor=lambda w: BeautifulSoup(w, 'lxml').get_text().lower(),\n",
    "                                 tokenizer=tokenizer.tokenize,\n",
    "                                 stop_words=nltk.corpus.stopwords.words('english'),  # words to be removed\n",
    "                                 lowercase=False,  # not need it as we already convert them\n",
    "                                 ngram_range=(1, 2),  # unigram\n",
    "                                 )\n",
    "        with open('vectorizer.pkl','rb') as f:\n",
    "            voc, idf = pickle.load(f)\n",
    "            self.vectorizer.vocabulary_ = voc\n",
    "            self.vectorizer.setIDF(idf)\n",
    "            self.vectorizer._tfidf._idf_diag = sp.spdiags(idf,      #again this to workaround sklearn limitation\n",
    "                                                     diags=0,\n",
    "                                                     m=len(idf),\n",
    "                                                     n=len(idf))\n",
    "\n",
    "    def analyzes(self, sentences):\n",
    "        '''\n",
    "        analyze more than one sentece\n",
    "        :return: the sentiment 1=positive, 0=negative\n",
    "        :param sentences: array of sentences\n",
    "        '''\n",
    "        return self.clf.predict(self.vectorizer.transform(sentences))\n",
    "\n",
    "    def analyze(self,sentence):\n",
    "        '''\n",
    "                analyze just one sentece\n",
    "                :return: the sentiment 1=positive, 0=negative\n",
    "                :param sentence: a string\n",
    "        '''\n",
    "        return self.analyzes([sentence])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SentimentAnalyzer().analyze('Sheldon too arrogant to be an actor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SentimentAnalyzer().analyze('Fares is not the best actor in the world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SentimentAnalyzer().analyze('I would rather spending my money on pizza rather than this')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
